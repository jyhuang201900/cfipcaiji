import requests
from bs4 import BeautifulSoup
import re
import os
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Set, List
import ipaddress

class IPCrawler:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Connection': 'keep-alive',
        }
        
        # æ‰©å±•çš„URLåˆ—è¡¨
        self.urls = [
            'https://ip.164746.xyz',
            'https://cf.090227.xyz',
            'https://stock.hostmonit.com/CloudFlareYes',
            'https://www.wetest.vip/page/cloudflare/address_v4.html',
            'https://monitor.gacjie.cn/page/cloudflare/ipv4.html',
            'https://ipdb.api.030101.xyz/?type=bestcf',
            'https://ipdb.api.030101.xyz/?type=bestproxy',
            'https://www.visa.cn',
            'https://www.visa.cn',
    'https://cf.877774.xyz',
    'https://ct.877774.xyz',
    'https://cmcc.877774.xyz',
    'https://cu.877774.xyz',
    'https://asia.877774.xyz',
    'https://eur.877774.xyz',
    'https://na.877774.xyz',
    'https://bpb.yousef.isegaro.com',
    
    
    'https://netlify-cname.xingpingcn.top'ï¼Œ
    'https://vercel.001315.xyz'ï¼Œ
    'https://vercel-cname.xingpingcn.top'ï¼Œ
    'https://cnamefuckxxs.yuchen.icu',
    'https://cdn.2020111.xyz'ï¼Œ
    'https://cf-cname.xingpingcn.top'ï¼Œ
    'https://cfcdn.v6.rocks'ï¼Œ
    'https://aliyun.2096.us.kg'ï¼Œ
    'https://cf.090227.xyz'ï¼Œ
    'https://time.cloudflare.com'ï¼Œ
    'https://checkout.shopify.com'ï¼Œ
    'https://shopify.com',
    'https://time.is',
    'https://icook.hk',
    'https://icook.tw'ï¼Œ
    'https://ip.sb',
    'https://japan.com',
    'https://malaysia.com',
    'https://russia.com',
    'https://singapore.com',
    'https://skk.moe',
    'https://www.visa.com',
    'https://www.visa.com.sg',
    'https://www.visa.com.hk',
    'https://www.visa.com.tw',
    'https://www.visa.co.jp',
    'https://www.visakorea.com',
    'https://www.gco.gov.qa',
    'https://www.gov.se',
    'https://www.gov.ua',
    'https://www.digitalocean.com',
    'https://www.csgo.com',
    'https://www.shopify.com',
    'https://www.whoer.net',
    'https://www.whatismyip.com',
    'https://www.ipget.net',
    'https://www.hugedomains.com',
    'https://www.udacity.com',
    'https://www.4chan.org',
    'https://www.okcupid.com',
    'https://www.glassdoor.com',
    'https://www.udemy.com',
    'https://www.baipiao.eu.org',
    'https://cdn.anycast.eu.org',
    
    
    'https://edgetunnel.anycast.eu.org',
    'https://alejandracaiccedo.com',
    'https://nc.gocada.co',
    'https://log.bpminecraft.com',
    'https://www.boba88slot.com',
    'https://gur.gov.ua',
    'https://www.zsu.gov.ua',
    'https://www.iakeys.com',
    'https://edtunnel-dgp.pages.dev',
    'https://www.d-555.com',
    'https://fbi.gov',
   
    'https://linux.do',
    'https://cloudflare.182682.xyz',
    'https://speed.marisalnc.com',
    'https://freeyx.cloudflare88.eu.org',
    'https://bestcf.top',
    'https://cfip.cfcdn.vip',
    'https://cf.0sm.com',
    'https://cf.zhetengsha.eu.org',
    'https://cloudflare.9jy.cc',
    'https://cf.zerone-cdn.pp.ua',
    'https://cfip.1323123.xyz',
    'https://cloudflare-ip.mofashi.ltd',
    'https://115155.xyz',
    'https://cname.xirancdn.us',
    'https://f3058171cad.002404.xyz',
    'https://8.889288.xyz',
    'https://cdn.tzpro.xyz',
    'https://cf.877771.xyz'
            
        ]
        
        # IPæ­£åˆ™è¡¨è¾¾å¼
        self.ip_pattern = r'\b(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\b'
        
        # IP:ç«¯å£æ ¼å¼çš„æ­£åˆ™è¡¨è¾¾å¼
        self.ip_port_pattern = r'\b(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?):\d{1,5}\b'
        
        self.unique_ips = set()
        self.unique_ip_ports = set()
    
    def is_valid_ip(self, ip: str) -> bool:
        """éªŒè¯IPåœ°å€æ˜¯å¦æœ‰æ•ˆä¸”ä¸ºå…¬ç½‘IP"""
        try:
            ip_obj = ipaddress.ip_address(ip)
            # æ’é™¤ç§æœ‰IPã€å›ç¯IPã€ä¿ç•™IPç­‰
            if ip_obj.is_private or ip_obj.is_loopback or ip_obj.is_reserved or ip_obj.is_multicast:
                return False
            # æ’é™¤ä¸€äº›ç‰¹æ®ŠèŒƒå›´
            if ip.startswith(('0.', '10.', '127.', '169.254.', '172.', '192.168.', '224.', '240.', '255.')):
                return False
            return True
        except ValueError:
            return False
    
    def extract_ips_from_html(self, html_content: str, url: str) -> Set[str]:
        """ä»HTMLå†…å®¹ä¸­æå–IPåœ°å€"""
        ips = set()
        
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # æ–¹æ³•1: æŸ¥æ‰¾æ‰€æœ‰æ–‡æœ¬å†…å®¹ä¸­çš„IP
            text_content = soup.get_text()
            ip_matches = re.findall(self.ip_pattern, text_content)
            
            # æ–¹æ³•2: ç‰¹åˆ«å¤„ç†è¡¨æ ¼æ•°æ®
            tables = soup.find_all('table')
            for table in tables:
                rows = table.find_all('tr')
                for row in rows:
                    cells = row.find_all(['td', 'th'])
                    for cell in cells:
                        cell_text = cell.get_text(strip=True)
                        cell_ips = re.findall(self.ip_pattern, cell_text)
                        ip_matches.extend(cell_ips)
            
            # æ–¹æ³•3: æŸ¥æ‰¾ç‰¹å®šæ ‡ç­¾
            for tag in soup.find_all(['span', 'div', 'p', 'code', 'pre', 'li']):
                tag_text = tag.get_text(strip=True)
                tag_ips = re.findall(self.ip_pattern, tag_text)
                ip_matches.extend(tag_ips)
            
            # æ–¹æ³•4: æå–IP:ç«¯å£æ ¼å¼
            ip_port_matches = re.findall(self.ip_port_pattern, text_content)
            for ip_port in ip_port_matches:
                ip = ip_port.split(':')[0]
                if self.is_valid_ip(ip):
                    self.unique_ip_ports.add(ip_port)
            
            # éªŒè¯å¹¶æ·»åŠ æœ‰æ•ˆçš„IP
            for ip in ip_matches:
                if self.is_valid_ip(ip):
                    ips.add(ip)
            
            print(f'âœ“ {url} - æ‰¾åˆ° {len(ips)} ä¸ªæœ‰æ•ˆIP')
            
        except Exception as e:
            print(f'âœ— è§£æ {url} çš„HTMLå†…å®¹å¤±è´¥: {e}')
        
        return ips
    
    def fetch_url(self, url: str, retry: int = 3) -> str:
        """è·å–URLå†…å®¹ï¼Œæ”¯æŒé‡è¯•"""
        for attempt in range(retry):
            try:
                response = requests.get(
                    url, 
                    headers=self.headers, 
                    timeout=10,
                    verify=False  # å¦‚æœé‡åˆ°SSLè¯ä¹¦é—®é¢˜
                )
                
                if response.status_code == 200:
                    return response.text
                else:
                    print(f'âš  {url} è¿”å›çŠ¶æ€ç : {response.status_code}')
                    
            except requests.exceptions.Timeout:
                print(f'â± {url} è¯·æ±‚è¶…æ—¶ (å°è¯• {attempt + 1}/{retry})')
            except requests.exceptions.ConnectionError:
                print(f'âš  {url} è¿æ¥å¤±è´¥ (å°è¯• {attempt + 1}/{retry})')
            except requests.exceptions.RequestException as e:
                print(f'âœ— {url} è¯·æ±‚å¤±è´¥: {e}')
                break
            
            if attempt < retry - 1:
                time.sleep(2)  # é‡è¯•å‰ç­‰å¾…
        
        return None
    
    def crawl_single_url(self, url: str) -> Set[str]:
        """çˆ¬å–å•ä¸ªURL"""
        print(f'ğŸ” æ­£åœ¨çˆ¬å–: {url}')
        html_content = self.fetch_url(url)
        
        if html_content:
            return self.extract_ips_from_html(html_content, url)
        return set()
    
    def crawl_all_urls(self, use_threading: bool = True, max_workers: int = 5):
        """çˆ¬å–æ‰€æœ‰URL"""
        print('=' * 60)
        print('å¼€å§‹çˆ¬å–IPåœ°å€...')
        print('=' * 60)
        
        if use_threading:
            # ä½¿ç”¨å¤šçº¿ç¨‹å¹¶å‘çˆ¬å–
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                future_to_url = {executor.submit(self.crawl_single_url, url): url for url in self.urls}
                
                for future in as_completed(future_to_url):
                    url = future_to_url[future]
                    try:
                        ips = future.result()
                        self.unique_ips.update(ips)
                    except Exception as e:
                        print(f'âœ— å¤„ç† {url} æ—¶å‡ºé”™: {e}')
        else:
            # ä¸²è¡Œçˆ¬å–
            for url in self.urls:
                ips = self.crawl_single_url(url)
                self.unique_ips.update(ips)
                time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
    
    def sort_ips(self, ips: Set[str]) -> List[str]:
        """æŒ‰IPåœ°å€çš„æ•°å­—é¡ºåºæ’åº"""
        return sorted(ips, key=lambda ip: [int(part) for part in ip.split('.')])
    
    def save_to_file(self, filename: str = 'ip.txt'):
        """ä¿å­˜IPåœ°å€åˆ°æ–‡ä»¶"""
        # åˆ é™¤æ—§æ–‡ä»¶
        if os.path.exists(filename):
            os.remove(filename)
        
        if self.unique_ips:
            sorted_ips = self.sort_ips(self.unique_ips)
            
            with open(filename, 'w', encoding='utf-8') as file:
                for ip in sorted_ips:
                    file.write(ip + '\n')
            
            print('=' * 60)
            print(f'âœ“ å·²ä¿å­˜ {len(sorted_ips)} ä¸ªå”¯ä¸€IPåœ°å€åˆ° {filename}')
            print('=' * 60)
            
            # æ˜¾ç¤ºå‰10ä¸ªIPä½œä¸ºç¤ºä¾‹
            print('\nå‰10ä¸ªIPåœ°å€:')
            for ip in sorted_ips[:10]:
                print(f'  â€¢ {ip}')
            if len(sorted_ips) > 10:
                print(f'  ... è¿˜æœ‰ {len(sorted_ips) - 10} ä¸ª')
        else:
            print('âœ— æœªæ‰¾åˆ°æœ‰æ•ˆçš„IPåœ°å€')
        
        # å¦‚æœæœ‰IP:ç«¯å£æ ¼å¼çš„æ•°æ®ï¼Œä¹Ÿä¿å­˜
        if self.unique_ip_ports:
            port_filename = 'ip_with_port.txt'
            if os.path.exists(port_filename):
                os.remove(port_filename)
            
            sorted_ip_ports = sorted(self.unique_ip_ports)
            with open(port_filename, 'w', encoding='utf-8') as file:
                for ip_port in sorted_ip_ports:
                    file.write(ip_port + '\n')
            
            print(f'\nâœ“ å·²ä¿å­˜ {len(sorted_ip_ports)} ä¸ªIP:ç«¯å£åˆ° {port_filename}')
    
    def add_custom_url(self, url: str):
        """æ·»åŠ è‡ªå®šä¹‰URL"""
        if url not in self.urls:
            self.urls.append(url)
            print(f'âœ“ å·²æ·»åŠ è‡ªå®šä¹‰URL: {url}')
    
    def run(self):
        """è¿è¡Œçˆ¬è™«"""
        start_time = time.time()
        
        # ç¦ç”¨SSLè­¦å‘Š
        import urllib3
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
        
        self.crawl_all_urls(use_threading=True, max_workers=5)
        self.save_to_file()
        
        elapsed_time = time.time() - start_time
        print(f'\nâ± æ€»è€—æ—¶: {elapsed_time:.2f} ç§’')


def main():
    crawler = IPCrawler()
    
    # å¯ä»¥æ·»åŠ æ›´å¤šè‡ªå®šä¹‰URL
    # crawler.add_custom_url('https://example.com/ips')
    
    crawler.run()


if __name__ == '__main__':
    main()
