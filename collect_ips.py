import requests
from bs4 import BeautifulSoup
import re
import os
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Set, List
import ipaddress
import warnings

# å¿½ç•¥SSLè­¦å‘Š
warnings.filterwarnings('ignore')

class IPCrawler:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Connection': 'keep-alive',
        }
        
        # æ‰©å±•çš„URLåˆ—è¡¨ï¼ˆå·²ä¿®å¤æ‰€æœ‰ä¸­æ–‡é€—å·ï¼‰
        self.urls = [
            'https://ip.164746.xyz',
            'https://cf.090227.xyz',
            'https://stock.hostmonit.com/CloudFlareYes',
            'https://www.wetest.vip/page/cloudflare/address_v4.html',
            'https://monitor.gacjie.cn/page/cloudflare/ipv4.html',
            'https://ipdb.api.030101.xyz/?type=bestcf',
            'https://ipdb.api.030101.xyz/?type=bestproxy',
            'https://www.visa.cn',
            'https://cf.877774.xyz',
            'https://ct.877774.xyz',
            'https://cmcc.877774.xyz',
            'https://cu.877774.xyz',
            'https://asia.877774.xyz',
            'https://eur.877774.xyz',
            'https://na.877774.xyz',
            'https://bpb.yousef.isegaro.com',
            'https://netlify-cname.xingpingcn.top',
            'https://vercel.001315.xyz',
            'https://vercel-cname.xingpingcn.top',
            'https://cnamefuckxxs.yuchen.icu',
            'https://cdn.2020111.xyz',
            'https://cf-cname.xingpingcn.top',
            'https://cfcdn.v6.rocks',
            'https://aliyun.2096.us.kg',
            'https://time.cloudflare.com',
            'https://checkout.shopify.com',
            'https://shopify.com',
            'https://time.is',
            'https://icook.hk',
            'https://icook.tw',
            'https://ip.sb',
            'https://japan.com',
            'https://malaysia.com',
            'https://russia.com',
            'https://singapore.com',
            'https://skk.moe',
            'https://www.visa.com',
            'https://www.visa.com.sg',
            'https://www.visa.com.hk',
            'https://www.visa.com.tw',
            'https://www.visa.co.jp',
            'https://www.visakorea.com',
            'https://www.gco.gov.qa',
            'https://www.gov.se',
            'https://www.gov.ua',
            'https://www.digitalocean.com',
            'https://www.csgo.com',
            'https://www.shopify.com',
            'https://www.whoer.net',
            'https://www.whatismyip.com',
            'https://www.ipget.net',
            'https://www.hugedomains.com',
            'https://www.udacity.com',
            'https://www.4chan.org',
            'https://www.okcupid.com',
            'https://www.glassdoor.com',
            'https://www.udemy.com',
            'https://www.baipiao.eu.org',
            'https://cdn.anycast.eu.org',
            'https://edgetunnel.anycast.eu.org',
            'https://alejandracaiccedo.com',
            'https://nc.gocada.co',
            'https://log.bpminecraft.com',
            'https://www.boba88slot.com',
            'https://gur.gov.ua',
            'https://www.zsu.gov.ua',
            'https://www.iakeys.com',
            'https://edtunnel-dgp.pages.dev',
            'https://www.d-555.com',
            'https://fbi.gov',
            'https://linux.do',
            'https://cloudflare.182682.xyz',
            'https://speed.marisalnc.com',
            'https://freeyx.cloudflare88.eu.org',
            'https://bestcf.top',
            'https://cfip.cfcdn.vip',
            'https://cf.0sm.com',
            'https://cf.zhetengsha.eu.org',
            'https://cloudflare.9jy.cc',
            'https://cf.zerone-cdn.pp.ua',
            'https://cfip.1323123.xyz',
            'https://cloudflare-ip.mofashi.ltd',
            'https://115155.xyz',
            'https://cname.xirancdn.us',
            'https://f3058171cad.002404.xyz',
            'https://8.889288.xyz',
            'https://cdn.tzpro.xyz',
            'https://cf.877771.xyz'
        ]
        
        # IPæ­£åˆ™è¡¨è¾¾å¼
        self.ip_pattern = r'\b(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\b'
        
        # IP:ç«¯å£æ ¼å¼çš„æ­£åˆ™è¡¨è¾¾å¼
        self.ip_port_pattern = r'\b(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?):\d{1,5}\b'
        
        self.unique_ips = set()
        self.unique_ip_ports = set()
        self.failed_urls = []
        self.success_count = 0
    
    def is_valid_ip(self, ip: str) -> bool:
        """éªŒè¯IPåœ°å€æ˜¯å¦æœ‰æ•ˆä¸”ä¸ºå…¬ç½‘IP"""
        try:
            ip_obj = ipaddress.ip_address(ip)
            # æ’é™¤ç§æœ‰IPã€å›ç¯IPã€ä¿ç•™IPç­‰
            if ip_obj.is_private or ip_obj.is_loopback or ip_obj.is_reserved or ip_obj.is_multicast:
                return False
            # æ’é™¤ä¸€äº›ç‰¹æ®ŠèŒƒå›´
            if ip.startswith(('0.', '10.', '127.', '169.254.', '172.', '192.168.', '224.', '240.', '255.')):
                return False
            # é¢å¤–æ’æŸ¥172æ®µçš„ç§æœ‰IP
            parts = ip.split('.')
            if parts[0] == '172' and 16 <= int(parts[1]) <= 31:
                return False
            return True
        except (ValueError, IndexError):
            return False
    
    def extract_ips_from_html(self, html_content: str, url: str) -> Set[str]:
        """ä»HTMLå†…å®¹ä¸­æå–IPåœ°å€"""
        ips = set()
        
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # æ–¹æ³•1: æŸ¥æ‰¾æ‰€æœ‰æ–‡æœ¬å†…å®¹ä¸­çš„IP
            text_content = soup.get_text()
            ip_matches = re.findall(self.ip_pattern, text_content)
            
            # æ–¹æ³•2: ç‰¹åˆ«å¤„ç†è¡¨æ ¼æ•°æ®
            tables = soup.find_all('table')
            for table in tables:
                rows = table.find_all('tr')
                for row in rows:
                    cells = row.find_all(['td', 'th'])
                    for cell in cells:
                        cell_text = cell.get_text(strip=True)
                        cell_ips = re.findall(self.ip_pattern, cell_text)
                        ip_matches.extend(cell_ips)
            
            # æ–¹æ³•3: æŸ¥æ‰¾ç‰¹å®šæ ‡ç­¾
            for tag in soup.find_all(['span', 'div', 'p', 'code', 'pre', 'li']):
                tag_text = tag.get_text(strip=True)
                tag_ips = re.findall(self.ip_pattern, tag_text)
                ip_matches.extend(tag_ips)
            
            # æ–¹æ³•4: æå–IP:ç«¯å£æ ¼å¼
            ip_port_matches = re.findall(self.ip_port_pattern, text_content)
            for ip_port in ip_port_matches:
                ip = ip_port.split(':')[0]
                if self.is_valid_ip(ip):
                    self.unique_ip_ports.add(ip_port)
            
            # éªŒè¯å¹¶æ·»åŠ æœ‰æ•ˆçš„IP
            for ip in ip_matches:
                if self.is_valid_ip(ip):
                    ips.add(ip)
            
            if ips:
                print(f'âœ“ {url[:60]}... - æ‰¾åˆ° {len(ips)} ä¸ªæœ‰æ•ˆIP')
            else:
                print(f'âŠ˜ {url[:60]}... - æœªæ‰¾åˆ°æœ‰æ•ˆIP')
            
        except Exception as e:
            print(f'âœ— è§£æ {url[:60]}... å¤±è´¥: {str(e)[:50]}')
        
        return ips
    
    def fetch_url(self, url: str, retry: int = 2) -> str:
        """è·å–URLå†…å®¹ï¼Œæ”¯æŒé‡è¯•"""
        for attempt in range(retry):
            try:
                response = requests.get(
                    url, 
                    headers=self.headers, 
                    timeout=15,
                    verify=False,
                    allow_redirects=True
                )
                
                if response.status_code == 200:
                    # å°è¯•æ£€æµ‹ç¼–ç 
                    response.encoding = response.apparent_encoding
                    return response.text
                else:
                    if attempt == retry - 1:
                        print(f'âš  {url[:60]}... è¿”å›çŠ¶æ€ç : {response.status_code}')
                    
            except requests.exceptions.Timeout:
                if attempt == retry - 1:
                    print(f'â± {url[:60]}... è¯·æ±‚è¶…æ—¶')
            except requests.exceptions.SSLError:
                if attempt == retry - 1:
                    print(f'ğŸ”’ {url[:60]}... SSLè¯ä¹¦é”™è¯¯')
            except requests.exceptions.ConnectionError:
                if attempt == retry - 1:
                    print(f'âš  {url[:60]}... è¿æ¥å¤±è´¥')
            except requests.exceptions.RequestException as e:
                if attempt == retry - 1:
                    print(f'âœ— {url[:60]}... è¯·æ±‚å¤±è´¥: {str(e)[:30]}')
                break
            except Exception as e:
                if attempt == retry - 1:
                    print(f'âœ— {url[:60]}... æœªçŸ¥é”™è¯¯: {str(e)[:30]}')
                break
            
            if attempt < retry - 1:
                time.sleep(1)
        
        return None
    
    def crawl_single_url(self, url: str) -> Set[str]:
        """çˆ¬å–å•ä¸ªURL"""
        print(f'ğŸ” æ­£åœ¨çˆ¬å–: {url[:70]}...')
        html_content = self.fetch_url(url)
        
        if html_content:
            self.success_count += 1
            return self.extract_ips_from_html(html_content, url)
        else:
            self.failed_urls.append(url)
            return set()
    
    def crawl_all_urls(self, use_threading: bool = True, max_workers: int = 10):
        """çˆ¬å–æ‰€æœ‰URL"""
        print('=' * 70)
        print(f'å¼€å§‹çˆ¬å–IPåœ°å€... (å…± {len(self.urls)} ä¸ªURL)')
        print('=' * 70)
        
        if use_threading:
            # ä½¿ç”¨å¤šçº¿ç¨‹å¹¶å‘çˆ¬å–
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                future_to_url = {executor.submit(self.crawl_single_url, url): url for url in self.urls}
                
                for future in as_completed(future_to_url):
                    url = future_to_url[future]
                    try:
                        ips = future.result()
                        self.unique_ips.update(ips)
                    except Exception as e:
                        print(f'âœ— å¤„ç† {url[:60]}... æ—¶å‡ºé”™: {str(e)[:50]}')
                        self.failed_urls.append(url)
        else:
            # ä¸²è¡Œçˆ¬å–
            for url in self.urls:
                ips = self.crawl_single_url(url)
                self.unique_ips.update(ips)
                time.sleep(0.5)
    
    def sort_ips(self, ips: Set[str]) -> List[str]:
        """æŒ‰IPåœ°å€çš„æ•°å­—é¡ºåºæ’åº"""
        try:
            return sorted(ips, key=lambda ip: [int(part) for part in ip.split('.')])
        except:
            return sorted(ips)
    
    def save_to_file(self, filename: str = 'ip.txt'):
        """ä¿å­˜IPåœ°å€åˆ°æ–‡ä»¶"""
        print('\n' + '=' * 70)
        print('ä¿å­˜ç»“æœ...')
        print('=' * 70)
        
        # åˆ é™¤æ—§æ–‡ä»¶
        if os.path.exists(filename):
            os.remove(filename)
        
        if self.unique_ips:
            sorted_ips = self.sort_ips(self.unique_ips)
            
            with open(filename, 'w', encoding='utf-8') as file:
                for ip in sorted_ips:
                    file.write(ip + '\n')
            
            print(f'âœ“ å·²ä¿å­˜ {len(sorted_ips)} ä¸ªå”¯ä¸€IPåœ°å€åˆ° {filename}')
            
            # æ˜¾ç¤ºå‰10ä¸ªIPä½œä¸ºç¤ºä¾‹
            print('\nğŸ“‹ å‰10ä¸ªIPåœ°å€:')
            for ip in sorted_ips[:10]:
                print(f'  â€¢ {ip}')
            if len(sorted_ips) > 10:
                print(f'  ... è¿˜æœ‰ {len(sorted_ips) - 10} ä¸ª')
        else:
            print('âœ— æœªæ‰¾åˆ°æœ‰æ•ˆçš„IPåœ°å€')
        
        # å¦‚æœæœ‰IP:ç«¯å£æ ¼å¼çš„æ•°æ®ï¼Œä¹Ÿä¿å­˜
        if self.unique_ip_ports:
            port_filename = 'ip_with_port.txt'
            if os.path.exists(port_filename):
                os.remove(port_filename)
            
            sorted_ip_ports = sorted(self.unique_ip_ports)
            with open(port_filename, 'w', encoding='utf-8') as file:
                for ip_port åœ¨ sorted_ip_ports:
                    file.write(ip_port + '\n')
            
            print(f'\nâœ“ å·²ä¿å­˜ {len(sorted_ip_ports)} ä¸ªIP:ç«¯å£åˆ° {port_filename}')
    
    def print_statistics(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        print('\n' + '=' * 70)
        print('ğŸ“Š ç»Ÿè®¡ä¿¡æ¯')
        print('=' * 70)
        print(f'æ€»URLæ•°é‡: {len(self.urls)}')
        print(f'æˆåŠŸçˆ¬å–: {self.success_count}')
        print(f'å¤±è´¥æ•°é‡: {len(self.failed_urls)}')
        print(f'å”¯ä¸€IPæ•°: {len(self.unique_ips)}')
        if self.unique_ip_ports:
            print(f'IP:ç«¯å£æ•°: {len(self.unique_ip_ports)}')
        
        if self.failed_urls:
            print(f'\nâŒ å¤±è´¥çš„URL (å…±{len(self.failed_urls)}ä¸ª):')
            for url in self.failed_urls[:5]:
                print(f'  â€¢ {url}')
            if len(self.failed_urls) > 5:
                print(f'  ... è¿˜æœ‰ {len(self.failed_urls) - 5} ä¸ª')
    
    def add_custom_url(self, url: str):
        """æ·»åŠ è‡ªå®šä¹‰URL"""
        if url not in self.urls:
            self.urlsã€‚append(url)
            print(f'âœ“ å·²æ·»åŠ è‡ªå®šä¹‰URL: {url}')
    
    def run(self):
        """è¿è¡Œçˆ¬è™«"""
        start_time = time.time()
        
        try:
            self.crawl_all_urls(use_threading=True, max_workers=10)
            self.save_to_file()
            self.print_statistics()
        except KeyboardInterrupt:
            print('\n\nâš  ç”¨æˆ·ä¸­æ–­ï¼Œæ­£åœ¨ä¿å­˜å·²è·å–çš„æ•°æ®...')
            self.save_to_file()
            self.print_statistics()
        except Exception as e:
            print(f'\n\nâŒ ç¨‹åºå‡ºé”™: {e}')
            if self.unique_ips:
                print('æ­£åœ¨ä¿å­˜å·²è·å–çš„æ•°æ®...')
                self.save_to_file()
        
        elapsed_time = time.time() - start_time
        print(f'\nâ± æ€»è€—æ—¶: {elapsed_time:.2f} ç§’')
        print('=' * 70)


def main():
    """ä¸»å‡½æ•°"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    IPåœ°å€æ‰¹é‡çˆ¬è™«å·¥å…·                          â•‘
â•‘                     Version 2.0                                â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    crawler = IPCrawler()
    
    # å¯ä»¥æ·»åŠ æ›´å¤šè‡ªå®šä¹‰URL
    # crawler.add_custom_url('https://example.com/ips')
    
    try:
        crawler.run()
    except Exception as e:
        print(f'\nâŒ ç¨‹åºå¼‚å¸¸é€€å‡º: {e}')
        print('è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–è”ç³»å¼€å‘è€…')


if __name__ == '__main__':
    main()
